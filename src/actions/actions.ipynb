{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "877f4837",
   "metadata": {},
   "source": [
    "# Iskanje delovanj\n",
    "V tem dokumentu predstavim uporabo diferencialnih enačb za iskanje delovanj in avtomorfizmov grafov.\n",
    "\n",
    "## Delovanja\n",
    "Iščemo model za homomorfizme $G=<S|R> \\to S_n$. Naivno bi lahko vsak generator $s$ slikali v poljubno tabelo $\\rho(s) = \\begin{pmatrix} \n",
    "1 & 2 & \\cdots & n \\\\\n",
    "1^s & 2^s & \\cdots & n^s \n",
    "\\end{pmatrix}$ in spreminjali vrednosti $1^s, 2^s, \\ldots, n^s $, da $\\rho$ postane homomorfziem. Težava je, da so vrednosti v tabeli diskretne in gradientne metode optimizacije odpadejo. \n",
    "\n",
    "Rešitev je, da na prostoru funckij $S_n$ (ali pa na $\\text{fun}([n], [n])$) uvedemo verjetnostno porazdelitev $P = P(\\phi)$ in optimiziramo $P(\\rho \\text{ je homomorfizem}$.\n",
    "\n",
    "Meni se zdita smiselna dva načina. Eden je bolj strojno učenjaški, eden pa je zelo podoben iskanju upodobitev."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697e8f90",
   "metadata": {},
   "source": [
    "## Matrični pristop\n",
    "Vzenimo  model $\\rho_\\phi \\colon G=<S|R> \\to \\text{dist}(\\text{fun}([n], [n]))$, ki je definiram s tem, da vsakemu generatorju $s \\in S$ dodeli matriko $\\phi_s \\in \\mathbb R^{n \\times n}$. *(Simetrično kot pri upodobitvah!!)*\n",
    "\n",
    "Matriko $\\phi_s$ pretvorimo v stohastično matriko $P_s=\\begin{bmatrix} s_{i,j} \\end{bmatrix}_{i, j = 1, \\dots, n} := \\text{softmax}_\\text{po vrsticah} (\\phi_s)$. \n",
    "\n",
    "S $P_s$ je določena porazdelitev nad $\\text{fun}([n], [n])$. Na $s \\in S$ lahko gledamo kot slučajno spremenljivko $s \\in  \\text{fun}([n], [n])$ in definiramo \n",
    "$$\n",
    "P(s(i) = j) := s_{i, j}.\n",
    "$$\n",
    "Za poljubno deterministično funckijo $f \\in \\text{fun}([n], [n])$ definiramo še \n",
    "$$\n",
    " P(s = f) = \\prod_{i=1}^n P(s(i) = f(i)) = \\prod_{i=1}^n s_{i, f(i)}.\n",
    "$$\n",
    "S tem smo definirali model $\\rho$, ki vsako matriko iz $\\{\\phi_s \\mid s \\in S\\}$ slika v svojo slučajno spremenljivko $s \\in \\text{fun}([n], [n])$. \n",
    "\n",
    "Želeli bi, da je **$P(\\rho \\text{ je homomorfizem in ima lepe lastnosti})$** čim večja."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c595a86",
   "metadata": {},
   "source": [
    "### $\\mathcal L_{rel}$ - kaznujmo modele, ki niso homomomorfizmi\n",
    "Naj bo $G=<S|R>$ in $r \\in R$ beseda v generatorjih iz $R$. Na vsak generator lahko gledamo kot na slučajno funkcijo nad $[n]$. Podobno lahko tudi na $r$ gledamo kot na kompozitum svojih generatorjev. Velja še, da je matrika porazdelitve za $r$ enaka $P_r = \\begin{bmatrix} P(r(i) = j) \\end{bmatrix}_{i,j} = \\prod _{s \\in r} P_s$ produkt matrik porazdelitev generatorjev v $r$. \n",
    "\n",
    "Želimo si, da je $r$ kot slučajna funkcija skoraj zagotovo enaka identiteti, torej da je \n",
    "$$\n",
    "P(r = \\text{id}) = 1.\n",
    "$$\n",
    "Ekvivalentno: \n",
    "$$\n",
    "0 = \\log(P(r = \\text{id})) = \\log(\\prod_{i=1}^n r_{i,i}) = \\sum_{i=1}^n \\log (r_{i,j}) = \\text{trace} (\\log(P_r)),\n",
    "$$\n",
    "kjer logaritem matrike računamo po elementih.\n",
    "Za funckijo izgube lahko vzamemo\n",
    "\n",
    "$\\mathcal L_{rel} =- \\text{trace} (\\log(P_r)) =-\n",
    " \\text{trace} (\\log(\\prod_{s \\in r}  \\text{softmax}(\\phi_s)  ))$.\n",
    "\n",
    "*Na nek način gre za maximal negative likelihood*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "81ea9f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parameters:  tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# torch \n",
    "import torch\n",
    "n = 3\n",
    "# Dn\n",
    "\n",
    "\n",
    "\n",
    "# za Dn --> Dn: \n",
    "phi = torch.tensor([[[0.19, 0.87, 0.43],\n",
    "         [0.58, 0.23, 0.97],\n",
    "         [0.57, 0.03, 1.00]],\n",
    "\n",
    "        [[0.48, 0.64, 0.04],\n",
    "         [0.47, 0.17, 0.84],\n",
    "         [0.63, 0.53, 0.74]]], requires_grad=True)\n",
    "\n",
    "\n",
    "# random : \n",
    "phi = torch.rand((2, n, n), requires_grad=True)\n",
    "phi = torch.ones((2, n, n), requires_grad=True)\n",
    "\n",
    "\n",
    "generators = {\n",
    "'r' : 0, 's' :1\n",
    "}\n",
    "relations = ['r'*n, 's'*2, 'rsrs']\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "def get_P(generator, phi):\n",
    "    return softmax(phi[generators[generator]])\n",
    "\n",
    "\n",
    "\n",
    "def loss_rel(generators, relations, phi):\n",
    "    ans =torch.zeros((n,n))\n",
    "    for r in relations:\n",
    "        prod = torch.eye(n)\n",
    "        for s in r:\n",
    "            prod = prod @ softmax(phi[generators[s]])\n",
    "        prod = torch.log(prod)\n",
    "        ans += prod \n",
    "    return -ans.trace() \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting parameters: \", phi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "46bfd366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 1: 9.887510299682617\n",
      "Loss at step 501: 9.887510299682617\n",
      "Loss at step 1001: 9.887510299682617\n",
      "Loss at step 1501: 9.887510299682617\n",
      "Loss at step 2001: 9.887510299682617\n",
      "Loss at step 2501: 9.887510299682617\n"
     ]
    }
   ],
   "source": [
    "# training \n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "n_steps = 3000\n",
    "opt = Adam([phi], lr= 0.01)\n",
    "#scheduler = StepLR(opt, step_size=30, gamma=0.1)\n",
    "\n",
    "for step in range(1, n_steps+1):\n",
    "    opt.zero_grad()\n",
    "    loss = loss_rel(generators, relations, phi)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "   # scheduler.step()\n",
    "    if step % 500 == 1:\n",
    "        print(f\"Loss at step {step}: {loss.item()}\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8fbea20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.33, 0.33, 0.33],\n",
       "         [0.33, 0.33, 0.33],\n",
       "         [0.33, 0.33, 0.33]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.33, 0.33, 0.33],\n",
       "         [0.33, 0.33, 0.33],\n",
       "         [0.33, 0.33, 0.33]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = get_P('r', phi)\n",
    "S = get_P('s', phi)\n",
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "R, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "464952d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({1: 1, 2: 2, 3: 3}, {1: 1, 2: 2, 3: 3})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def closest_permutation_matrix(M: torch.Tensor) -> torch.Tensor:\n",
    "    # Convert to numpy for scipy\n",
    "    M_np = M.detach().numpy()\n",
    "\n",
    "    # Solve the linear sum assignment problem on the NEGATED matrix\n",
    "    # (because we want to maximize the sum, but scipy minimizes)\n",
    "    row_ind, col_ind = linear_sum_assignment(-M_np)\n",
    "\n",
    "    # Create the permutation matrix\n",
    "    n = M.size(0)\n",
    "    P = torch.zeros_like(M)\n",
    "    P[row_ind, col_ind] = 1.0\n",
    "    return P\n",
    "\n",
    "def permutation_from_matrix(X: torch.Tensor) -> torch.Tensor:\n",
    "    table =X.argmax(dim=1) + 1 \n",
    "    perm = {}\n",
    "    for i in range(X.shape[0]):\n",
    "        perm[i+1] = table[i].item()\n",
    "    return perm\n",
    "\n",
    "\n",
    "permutation_from_matrix(closest_permutation_matrix(R)), permutation_from_matrix(closest_permutation_matrix(S))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df33d7a",
   "metadata": {},
   "source": [
    "### $\\mathcal L_{perm}$ - kaznujmo modele, ki niso permutacije\n",
    "Podobno kot pri upodobitvah, ko definiramo funckijo izgube $\\mathcal L_{unit}$, ki matrike optimizira v unitarne in s tem v obrnljive, lahko tukaj pomagamo $\\mathcal L_{rel}$ s tem, da funckije \"potiskamo\" v bijekcije. \n",
    "\n",
    "Naj bo $s \\in S$ generator. Radi bi, da je $s$ kot preslikava nad $[n]$ skoraj zagotovo bijekcija, torej \n",
    "$$\n",
    "P(s \\in S_n )= 1.\n",
    "$$\n",
    "Velja pa \n",
    "$$\n",
    "P(s \\in S_n )= \\sum_{\\sigma \\in S_n} P(s = \\sigma) = \\sum_{\\sigma \\in S_n}   \\prod_{i=1}^n s_{i, \\sigma(i)}\n",
    "= \\text{Perm} (P_s).\n",
    "$$\n",
    "[Edine stohastične matrike z enotskim permanentom so permutacijske](https://math.stackexchange.com/questions/5063254/if-a-stochastic-matrix-has-unit-permanent-is-it-a-permutation-matrix), torej je \n",
    "$\n",
    "P(s \\in S_n )= 1.\n",
    "$ natantko tedaj, ko je $P_s$ permutacijska. Za stohastične matrike pa je to ekvivalentno temu, da je $P_s$ **unitarna**. \n",
    "\n",
    "Za funkcijo izgube lahko vzamemo kar funckijo izgube za unitarnost, uporabljeno na matrikah $P_s$:\n",
    "$$\n",
    "\\mathcal L_{perm} = \\sum_{s \\in S}||P_sP_s^*  - I||_F^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b79e21",
   "metadata": {},
   "source": [
    "### Avtomorfizmi grafov \n",
    "Naj bo $\\mathcal G = ([n], E)$ graf na $n$ vozljiščih in $M=\\begin{bmatrix}m_{i,j}\\end{bmatrix}$ njegova matrika sosednosti. Naš model $\\rho_\\phi$ lahko spreminjamo v avtomorfizem grafa $\\mathcal G$.\n",
    "\n",
    "Za $(i,j) \\in E$ računamo \n",
    "$$\n",
    "P(i^s \\sim j ^s) = \\sum_{k= 1}^n \\sum_{h = 1}^n s_{i, k} s_{j, h} m_{k,h} = s_j^T M s_i,\n",
    "$$\n",
    "kjer je $P_s = \\begin{bmatrix} s_1^T \\\\ \\vdots \\\\ s_n^T \\end{bmatrix}$.\n",
    "\n",
    "Radi bi, da velja \n",
    "$\n",
    "1 = \\displaystyle\\prod_{(i,j) \\in E} P(i^s \\sim j ^s).\n",
    "$. \n",
    "Za funkcijo izgube lahko vzamemo\n",
    "$$\n",
    "\\mathcal L_{aut} = -\\sum_{i=1}^n\\sum_{j=1}^n log(s_j^T M s_i)m_{i,j}= -\\text{tr}(\\log(P_sMP_s^T) M^T),\n",
    "$$\n",
    "kjer je $\\log(SMS^T)$ izračunan po elementih."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df203259",
   "metadata": {},
   "source": [
    "Podoben premislek dela za poljubne morfizme grafov. Naj bo $M$ matrika sosednosti enega grafa in $N$ matrika sosednosti drugega. Velja $P(i^s \\sim j ^s) = \\sum_{k= 1}^n \\sum_{h = 1}^n s_{i, k} s_{j, h} n_{k,h} = s_j^T N s_i$. Za loss izberemo \n",
    "$$\n",
    "\\mathcal L_{aut} = -\\sum_{i=1}^n\\sum_{j=1}^n log(s_j^T N s_i)m_{i,j}= -\\text{tr}(\\log(P_sNP_s^T) M^T)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd795bb2",
   "metadata": {},
   "source": [
    "### Avtomorfizmi grafov - brez group\n",
    "Verjetno je bolj smiselno iskati le avtomorfizme grafov. Vse isto, le pozabiš na $G$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b20b236",
   "metadata": {},
   "source": [
    "Demo: poiščimo kak avtomorfizem preporostega grafa:\n",
    "![Demo graf](demo_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7087724e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2), (0, 3), (0, 4), (1, 2), (1, 3), (1, 5), (2, 5), (3, 4), (4, 5)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def compose(p, q):\n",
    "    \"\"\"Return the composition p ∘ q (first q, then p) for permutations\n",
    "    stored as tuples whose i-th entry is the image of i+1.\"\"\"\n",
    "    return tuple(p[q[i] - 1] for i in range(len(p)))\n",
    "\n",
    "def inverse(p):\n",
    "    \"\"\"Inverse of a permutation given in tuple form.\"\"\"\n",
    "    inv = [0] * len(p)\n",
    "    for i, x in enumerate(p):\n",
    "        inv[x - 1] = i + 1\n",
    "    return tuple(inv)\n",
    "\n",
    "def cayley_Sn_edges(n: int):\n",
    "    \"\"\"\n",
    "    Return the edge list of the (undirected) Cayley graph of S_n\n",
    "    with generators a = (1 2) and b = (1 2 … n).  Vertices are\n",
    "    indexed 0 … n!−1 in lexicographic order of permutations.\n",
    "    Each edge is an ordered pair (i, j) with i < j.\n",
    "    \"\"\"\n",
    "    if n < 2:\n",
    "        raise ValueError(\"n must be at least 2\")\n",
    "\n",
    "    # all permutations of 1…n in lexicographic order\n",
    "    perms = list(itertools.permutations(range(1, n + 1)))\n",
    "    index = {p: k for k, p in enumerate(perms)}\n",
    "\n",
    "    # generators\n",
    "    a = tuple([2, 1, *range(3, n + 1)])          # (1 2)\n",
    "    b = tuple([*range(2, n + 1), 1])             # (1 2 … n)\n",
    "    b_inv = inverse(b)                           # (1 n … 2)\n",
    "\n",
    "    edges = set()\n",
    "    for p in perms:\n",
    "        i = index[p]\n",
    "        for g in (a, b, b_inv):\n",
    "            j = index[compose(g, p)]\n",
    "            if i < j:\n",
    "                edges.add((i, j))\n",
    "\n",
    "    # return a list sorted for reproducibility\n",
    "    return sorted(edges)\n",
    "\n",
    "cayley_Sn_edges(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6cc71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "906817f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial matrix:  tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], requires_grad=True)\n",
      "Loss at step 1: 8.545177459716797\n",
      "Loss at step 501: 4.105795860290527\n",
      "Loss at step 1001: 4.0279669761657715\n",
      "Loss at step 1501: 4.0125956535339355\n",
      "Loss at step 2001: 4.006472587585449\n",
      "Loss at step 2501: 4.003348350524902\n"
     ]
    }
   ],
   "source": [
    "# torch\n",
    "\n",
    "m=6\n",
    "m=4\n",
    "# prepare small graph\n",
    "edges = [(0,1), (0,2), (1,2), (2, 3)]\n",
    "\n",
    "#edges = cayley_Sn_edges(3)\n",
    "M = torch.zeros((m,m))\n",
    "for i, j in edges:\n",
    "    M[i,j] = 1\n",
    "    M[j, i] = 1\n",
    "\n",
    "# primer, ko skonvergiramo:\n",
    "Q = torch.tensor([[0.13, 0.34, 0.38, 0.23],\n",
    "        [0.71, 0.22, 0.98, 0.84],\n",
    "        [0.25, 0.52, 0.71, 0.59],\n",
    "        [0.85, 0.93, 0.88, 0.64]], requires_grad=True)\n",
    "\n",
    "\n",
    "# input:\n",
    "Q = torch.rand((m,m), requires_grad=True)\n",
    "Q  =torch.ones((m,m), requires_grad=True)\n",
    "\n",
    "def loss_aut(Q, M):\n",
    "    P = softmax(Q)\n",
    "    ans = P @ M @ P.transpose(0,1)\n",
    "    ans = torch.log(ans )\n",
    "    ans = ans @ M.transpose(0,1)\n",
    "    return -ans.trace()\n",
    "\n",
    "def loss_bijection(Q):\n",
    "    P = softmax(Q)\n",
    "    return torch.linalg.matrix_norm( P@ P.transpose(0,1) - torch.eye(P.shape[0]))**2\n",
    "\n",
    "\n",
    "n_steps_aut = 3000\n",
    "opt_aut = Adam([Q], lr=0.01)\n",
    "\n",
    "print(\"Initial matrix: \", Q)\n",
    "\n",
    "for step in range(1, n_steps_aut+1):\n",
    "    opt_aut.zero_grad()\n",
    "    loss =loss_aut(Q, M) + loss_bijection(Q)\n",
    "    loss.backward()\n",
    "    opt_aut.step()\n",
    "    if step % 500 == 1:\n",
    "        print(f\"Loss at step {step}: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6d7587a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    0.00,     0.00,     1.00,     0.00],\n",
       "        [    0.50,     0.50,     0.00,     0.00],\n",
       "        [    0.50,     0.50,     0.00,     0.00],\n",
       "        [    0.15,     0.15,     0.70,     0.00]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.imshow(softmax(Q).detach().numpy(), cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title(\"Q matrix\")\n",
    "plt.show()\n",
    "softmax(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "44207700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permutation_from_matrix(closest_permutation_matrix(Q.transpose(0,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26c7b92",
   "metadata": {},
   "source": [
    "## Strojnoučenjaški pristop\n",
    "Namesto, da optimiziramo model $\\rho \\colon G \\to \\text{fun}([n], [n])$, naredimo model, ki že v začetku slika v $S_n$. (Podobno, kot smo pri upodobitvah naredili model, ki direktno slika v O(2)).\n",
    "\n",
    "Velja $S_n = <a, b | R>$. Vsaka permutacija je torej beseda v $\\{a, b\\}$. \n",
    "\n",
    "Besede lahko gradimo rekurzivno. Definiramo model $M : S_n \\to \\text{bernulli}\\{a, b\\}$, ki za vsako permutacijo $\\pi \\in S_n$ vrne vektor verjetnosti  $[P(a | \\pi), P(b | \\pi), P(id | \\pi)]$. S pomočjo tega modela lahko gradimo markovsko verigo:\n",
    "- začneš z identiteto $\\pi_0 = \\text{id}$\n",
    "- vsak korak iz porazdelitve  $[P(a | \\pi_i), P(b | \\pi_i), P(id | \\pi)]$ vzorčiš generator $g \\in \\{a, b\\}$. Če je $g$ identiteta, končaš in vrneš $\\pi _i$, sicer pa nastaviš $\\pi_{i+1} = \\text{perm}(\\pi_i \\circ g)$,\n",
    "kjer je $\\text{perm}$ preslikava, ki besede slika v permutacije, ki jih besede predstavljajo (to je boljše, kot da model za vhod vzame besedo - besede so ppoljubno dolge in redundantne, permutacija pa je  vektor dimenzije $n$).\n",
    "\n",
    "Za poljubno funckijo izgube $\\mathcal L$ nad preslikavami  $G \\mapsto S_n$ lahko minimaliziramo $E[\\mathcal L (\\rho)]$, kjer je $\\rho$ zgoraj opisani slučjani proces.\n",
    "\n",
    "Ta pristop je kul, ker se je z njim (v obliki $S \\to aS  |bs | 1$) vse začelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
