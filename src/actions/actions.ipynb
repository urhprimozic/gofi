{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "877f4837",
   "metadata": {},
   "source": [
    "# Iskanje delovanj\n",
    "V tem dokumentu predstavim uporabo diferencialnih enačb za iskanje delovanj in avtomorfizmov grafov.\n",
    "\n",
    "## Delovanja\n",
    "Iščemo model za homomorfizme $G=<S|R> \\to S_n$. Naivno bi lahko vsak generator $s$ slikali v poljubno tabelo $\\rho(s) = \\begin{pmatrix} \n",
    "1 & 2 & \\cdots & n \\\\\n",
    "1^s & 2^s & \\cdots & n^s \n",
    "\\end{pmatrix}$ in spreminjali vrednosti $1^s, 2^s, \\ldots, n^s $, da $\\rho$ postane homomorfziem. Težava je, da so vrednosti v tabeli diskretne in gradientne metode optimizacije odpadejo. \n",
    "\n",
    "Rešitev je, da na prostoru funckij $S_n$ (ali pa na $\\text{fun}([n], [n])$) uvedemo verjetnostno porazdelitev $P = P(\\phi)$ in optimiziramo $P(\\rho \\text{ je homomorfizem}$.\n",
    "\n",
    "Meni se zdita smiselna dva načina. Eden je bolj strojno učenjaški, eden pa je zelo podoben iskanju upodobitev."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697e8f90",
   "metadata": {},
   "source": [
    "## Matrični pristop\n",
    "Vzenimo  model $\\rho_\\phi \\colon G=<S|R> \\to \\text{dist}(\\text{fun}([n], [n]))$, ki je definiram s tem, da vsakemu generatorju $s \\in S$ dodeli matriko $\\phi_s \\in \\mathbb R^{n \\times n}$. *(Simetrično kot pri upodobitvah!!)*\n",
    "\n",
    "Matriko $\\phi_s$ pretvorimo v stohastično matriko $P_s=\\begin{bmatrix} s_{i,j} \\end{bmatrix}_{i, j = 1, \\dots, n} := \\text{softmax}_\\text{po vrsticah} (\\phi_s)$. \n",
    "\n",
    "S $P_s$ je določena porazdelitev nad $\\text{fun}([n], [n])$. Na $s \\in S$ lahko gledamo kot slučajno spremenljivko $s \\in  \\text{fun}([n], [n])$ in definiramo \n",
    "$$\n",
    "P(s(i) = j) := s_{i, j}.\n",
    "$$\n",
    "Za poljubno deterministično funckijo $f \\in \\text{fun}([n], [n])$ definiramo še \n",
    "$$\n",
    " P(s = f) = \\prod_{i=1}^n P(s(i) = f(i)) = \\prod_{i=1}^n s_{i, f(i)}.\n",
    "$$\n",
    "S tem smo definirali model $\\rho$, ki vsako matriko iz $\\{\\phi_s \\mid s \\in S\\}$ slika v svojo slučajno spremenljivko $s \\in \\text{fun}([n], [n])$. \n",
    "\n",
    "Želeli bi, da je **$P(\\rho \\text{ je homomorfizem in ima lepe lastnosti})$** čim večja."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c595a86",
   "metadata": {},
   "source": [
    "### $\\mathcal L_{rel}$ - kaznujmo modele, ki niso homomomorfizmi\n",
    "Naj bo $G=<S|R>$ in $r \\in R$ beseda v generatorjih iz $R$. Na vsak generator lahko gledamo kot na slučajno funkcijo nad $[n]$. Podobno lahko tudi na $r$ gledamo kot na kompozitum svojih generatorjev. Velja še, da je matrika porazdelitve za $r$ enaka $P_r = \\begin{bmatrix} P(r(i) = j) \\end{bmatrix}_{i,j} = \\prod _{s \\in r} P_s$ produkt matrik porazdelitev generatorjev v $r$. \n",
    "\n",
    "Želimo si, da je $r$ kot slučajna funkcija skoraj zagotovo enaka identiteti, torej da je \n",
    "$$\n",
    "P(r = \\text{id}) = 1.\n",
    "$$\n",
    "Ekvivalentno: \n",
    "$$\n",
    "0 = \\log(P(r = \\text{id})) = \\log(\\prod_{i=1}^n r_{i,i}) = \\sum_{i=1}^n \\log (r_{i,j}) = \\text{trace} (\\log(P_r)),\n",
    "$$\n",
    "kjer logaritem matrike računamo po elementih.\n",
    "Za funckijo izgube lahko vzamemo\n",
    "\n",
    "$\\mathcal L_{rel} =- \\text{trace} (\\log(P_r)) =-\n",
    " \\text{trace} (\\log(\\prod_{s \\in r}  \\text{softmax}(\\phi_s)  ))$.\n",
    "\n",
    "*Na nek način gre za maximal negative likelihood*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81ea9f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parameters:  tensor([[[0.2370, 0.3730, 0.4659],\n",
      "         [0.6785, 0.4033, 0.9303],\n",
      "         [0.0942, 0.6956, 0.5490]],\n",
      "\n",
      "        [[0.5581, 0.7205, 0.8317],\n",
      "         [0.8713, 0.0569, 0.8304],\n",
      "         [0.3633, 0.5012, 0.4941]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# torch \n",
    "import torch\n",
    "n = 3\n",
    "\n",
    "phi = torch.rand((2, n, n), requires_grad=True)\n",
    "generators = {\n",
    "'r' : 0, 's' :1\n",
    "}\n",
    "relations = ['r'*n, 's'*2, 'rsrs']\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "def get_P(generator, phi):\n",
    "    return softmax(phi[generators[generator]])\n",
    "\n",
    "\n",
    "\n",
    "def loss_rel(generators, relations, phi):\n",
    "    ans =torch.zeros((n,n))\n",
    "    for r in relations:\n",
    "        prod = torch.eye(n)\n",
    "        for s in r:\n",
    "            prod = prod @ softmax(phi[generators[s]])\n",
    "        prod = torch.log(prod)\n",
    "        ans += prod \n",
    "    return -ans.trace() \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting parameters: \", phi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46bfd366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 1: 9.86717700958252\n",
      "Loss at step 101: 7.708000183105469\n",
      "Loss at step 201: 2.2608485221862793\n",
      "Loss at step 301: 0.5483191609382629\n",
      "Loss at step 401: 0.2948185205459595\n",
      "Loss at step 501: 0.1931959092617035\n",
      "Loss at step 601: 0.13924869894981384\n",
      "Loss at step 701: 0.10628984868526459\n",
      "Loss at step 801: 0.08433441817760468\n",
      "Loss at step 901: 0.06881647557020187\n"
     ]
    }
   ],
   "source": [
    "# training \n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "n_steps = 1000\n",
    "opt = Adam([phi], lr= 0.01)\n",
    "#scheduler = StepLR(opt, step_size=30, gamma=0.1)\n",
    "\n",
    "for step in range(1, n_steps+1):\n",
    "    opt.zero_grad()\n",
    "    loss = loss_rel(generators, relations, phi)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "   # scheduler.step()\n",
    "    if step % 100 == 1:\n",
    "        print(f\"Loss at step {step}: {loss.item()}\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8fbea20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[8.0666e-04, 8.5011e-04, 9.9834e-01],\n",
       "         [9.9843e-01, 6.1860e-04, 9.4981e-04],\n",
       "         [7.1738e-04, 9.9852e-01, 7.5958e-04]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[9.9434e-01, 4.4166e-03, 1.2438e-03],\n",
       "         [1.1766e-03, 4.8253e-04, 9.9834e-01],\n",
       "         [3.3516e-04, 9.9886e-01, 8.0440e-04]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = get_P('r', phi)\n",
    "S = get_P('s', phi)\n",
    "R, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464952d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     P[row_ind, col_ind] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m P\n\u001b[0;32m---> 17\u001b[0m \u001b[43mclosest_permutation_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mR\u001b[49m\u001b[43m)\u001b[49m, closest_permutation_matrix(S)\n",
      "Cell \u001b[0;32mIn[38], line 5\u001b[0m, in \u001b[0;36mclosest_permutation_matrix\u001b[0;34m(M)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosest_permutation_matrix\u001b[39m(M: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Convert to numpy for scipy\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     M_np \u001b[38;5;241m=\u001b[39m \u001b[43mM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Solve the linear sum assignment problem on the NEGATED matrix\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# (because we want to maximize the sum, but scipy minimizes)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     row_ind, col_ind \u001b[38;5;241m=\u001b[39m linear_sum_assignment(\u001b[38;5;241m-\u001b[39mM_np)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def closest_permutation_matrix(M: torch.Tensor) -> torch.Tensor:\n",
    "    # Convert to numpy for scipy\n",
    "    M_np = M.detach().numpy()\n",
    "\n",
    "    # Solve the linear sum assignment problem on the NEGATED matrix\n",
    "    # (because we want to maximize the sum, but scipy minimizes)\n",
    "    row_ind, col_ind = linear_sum_assignment(-M_np)\n",
    "\n",
    "    # Create the permutation matrix\n",
    "    n = M.size(0)\n",
    "    P = torch.zeros_like(M)\n",
    "    P[row_ind, col_ind] = 1.0\n",
    "    return P\n",
    "\n",
    "closest_permutation_matrix(R), closest_permutation_matrix(S)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df33d7a",
   "metadata": {},
   "source": [
    "### $\\mathcal L_{perm}$ - kaznujmo modele, ki niso permutacije\n",
    "Podobno kot pri upodobitvah, ko definiramo funckijo izgube $\\mathcal L_{unit}$, ki matrike optimizira v unitarne in s tem v obrnljive, lahko tukaj pomagamo $\\mathcal L_{rel}$ s tem, da funckije \"potiskamo\" v bijekcije. \n",
    "\n",
    "Naj bo $s \\in S$ generator. Radi bi, da je $s$ kot preslikava nad $[n]$ skoraj zagotovo bijekcija, torej \n",
    "$$\n",
    "P(s \\in S_n )= 1.\n",
    "$$\n",
    "Velja pa \n",
    "$$\n",
    "P(s \\in S_n )= \\sum_{\\sigma \\in S_n} P(s = \\sigma) = \\sum_{\\sigma \\in S_n}   \\prod_{i=1}^n s_{i, \\sigma(i)}\n",
    "= \\text{Perm} (P_s).\n",
    "$$\n",
    "[Edine stohastične matrike z enotskim permanentom so permutacijske](https://math.stackexchange.com/questions/5063254/if-a-stochastic-matrix-has-unit-permanent-is-it-a-permutation-matrix), torej je \n",
    "$\n",
    "P(s \\in S_n )= 1.\n",
    "$ natantko tedaj, ko je $P_s$ permutacijska. Za stohastične matrike pa je to ekvivalentno temu, da je $P_s$ **unitarna**. \n",
    "\n",
    "Za funkcijo izgube lahko vzamemo kar funckijo izgube za unitarnost, uporabljeno na matrikah $P_s$:\n",
    "$$\n",
    "\\mathcal L_{perm} = \\sum_{s \\in S}||P_sP_s^*  - I||_F^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b79e21",
   "metadata": {},
   "source": [
    "### Avtomorfizmi grafov \n",
    "Naj bo $\\mathcal G = ([n], E)$ graf na $n$ vozljiščih in $M=\\begin{bmatrix}m_{i,j}\\end{bmatrix}$ njegova matrika sosednosti. Naš model $\\rho_\\phi$ lahko spreminjamo v avtomorfizem grafa $\\mathcal G$.\n",
    "\n",
    "Za $(i,j) \\in E$ računamo \n",
    "$$\n",
    "P(i^s \\sim j ^s) = \\sum_{k= 1}^n \\sum_{h = 1}^n s_{i, k} s_{j, h} m_{k,h} = s_j^T M s_i,\n",
    "$$\n",
    "kjer je $P_s = \\begin{bmatrix} s_1^T \\\\ \\vdots \\\\ s_n^T \\end{bmatrix}$.\n",
    "\n",
    "Radi bi, da velja \n",
    "$\n",
    "1 = \\displaystyle\\prod_{(i,j) \\in E} P(i^s \\sim j ^s).\n",
    "$. \n",
    "Za funkcijo izgube lahko vzamemo\n",
    "$$\n",
    "\\mathcal L_{aut} = \\sum_{i=1}^n\\sum_{j=1}^n log(s_j^T M s_i)m_{i,j}= \\text{tr}(\\log(P_sMP_s^T) M^T),\n",
    "$$\n",
    "kjer je $\\log(SMS^T)$ izračunan po elementih."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd795bb2",
   "metadata": {},
   "source": [
    "### Avtomorfizmi grafov - brez group\n",
    "Verjetno je bolj smiselno iskati le avtomorfizme grafov. Vse isto, le pozabiš na $G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "906817f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# torch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "# torch\n",
    "import torch\n",
    "\n",
    "X = torch.random()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26c7b92",
   "metadata": {},
   "source": [
    "## Strojnoučenjaški pristop\n",
    "Namesto, da optimiziramo model $\\rho \\colon G \\to \\text{fun}([n], [n])$, naredimo model, ki že v začetku slika v $S_n$. (Podobno, kot smo pri upodobitvah naredili model, ki direktno slika v O(2)).\n",
    "\n",
    "Velja $S_n = <a, b | R>$. Vsaka permutacija je torej beseda v $\\{a, b\\}$. \n",
    "\n",
    "Besede lahko gradimo rekurzivno. Definiramo model $M : S_n \\to \\text{bernulli}\\{a, b\\}$, ki za vsako permutacijo $\\pi \\in S_n$ vrne vektor verjetnosti  $[P(a | \\pi), P(b | \\pi), P(id | \\pi)]$. S pomočjo tega modela lahko gradimo markovsko verigo:\n",
    "- začneš z identiteto $\\pi_0 = \\text{id}$\n",
    "- vsak korak iz porazdelitve  $[P(a | \\pi_i), P(b | \\pi_i), P(id | \\pi)]$ vzorčiš generator $g \\in \\{a, b\\}$. Če je $g$ identiteta, končaš in vrneš $\\pi _i$, sicer pa nastaviš $\\pi_{i+1} = \\text{perm}(\\pi_i \\circ g)$,\n",
    "kjer je $\\text{perm}$ preslikava, ki besede slika v permutacije, ki jih besede predstavljajo (to je boljše, kot da model za vhod vzame besedo - besede so ppoljubno dolge in redundantne, permutacija pa je  vektor dimenzije $n$).\n",
    "\n",
    "Za poljubno funckijo izgube $\\mathcal L$ nad preslikavami  $G \\mapsto S_n$ lahko minimaliziramo $E[\\mathcal L (\\rho)]$, kjer je $\\rho$ zgoraj opisani slučjani proces.\n",
    "\n",
    "Ta pristop je kul, ker se je z njim (v obliki $S \\to aS  |bs | 1$) vse začelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
