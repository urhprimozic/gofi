{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "877f4837",
   "metadata": {},
   "source": [
    "# Iskanje delovanj\n",
    "V tem dokumentu predstavim uporabo diferencialnih enačb za iskanje delovanj in avtomorfizmov grafov.\n",
    "\n",
    "## Delovanja\n",
    "Iščemo model za homomorfizme $G=<S|R> \\to S_n$. Naivno bi lahko vsak generator $s$ slikali v poljubno tabelo $\\rho(s) = \\begin{pmatrix} \n",
    "1 & 2 & \\cdots & n \\\\\n",
    "1^s & 2^s & \\cdots & n^s \n",
    "\\end{pmatrix}$ in spreminjali vrednosti $1^s, 2^s, \\ldots, n^s $, da $\\rho$ postane homomorfziem. Težava je, da so vrednosti v tabeli diskretne in gradientne metode optimizacije odpadejo. \n",
    "\n",
    "Rešitev je, da na prostoru funckij $S_n$ (ali pa na $\\text{fun}([n], [n])$) uvedemo verjetnostno porazdelitev $P = P(\\phi)$ in optimiziramo $P(\\rho \\text{ je homomorfizem}$.\n",
    "\n",
    "Meni se zdita smiselna dva načina. Eden je bolj strojno učenjaški, eden pa je zelo podoben iskanju upodobitev."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697e8f90",
   "metadata": {},
   "source": [
    "## Matrični pristop\n",
    "Vzenimo  model $\\rho_\\phi \\colon G=<S|R> \\to \\text{dist}(\\text{fun}([n], [n]))$, ki je definiram s tem, da vsakemu generatorju $s \\in S$ dodeli matriko $\\phi_s \\in \\mathbb R^{n \\times n}$. *(Simetrično kot pri upodobitvah!!)*\n",
    "\n",
    "Matriko $\\phi_s$ pretvorimo v stohastično matriko $P_s=\\begin{bmatrix} s_{i,j} \\end{bmatrix}_{i, j = 1, \\dots, n} := \\text{softmax}_\\text{po vrsticah} (\\phi_s)$. \n",
    "\n",
    "S $P_s$ je določena porazdelitev nad $\\text{fun}([n], [n])$. Na $s \\in S$ lahko gledamo kot slučajno spremenljivko $s \\in  \\text{fun}([n], [n])$ in definiramo \n",
    "$$\n",
    "P(s(i) = j) := s_{i, j}.\n",
    "$$\n",
    "Za poljubno deterministično funckijo $f \\in \\text{fun}([n], [n])$ definiramo še \n",
    "$$\n",
    " P(s = f) = \\prod_{i=1}^n P(s(i) = f(i)) = \\prod_{i=1}^n s_{i, f(i)}.\n",
    "$$\n",
    "S tem smo definirali model $\\rho$, ki vsako matriko iz $\\{\\phi_s \\mid s \\in S\\}$ slika v svojo slučajno spremenljivko $s \\in \\text{fun}([n], [n])$. \n",
    "\n",
    "Želeli bi, da je **$P(\\rho \\text{ je homomorfizem in ima lepe lastnosti})$** čim večja."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c595a86",
   "metadata": {},
   "source": [
    "### $\\mathcal L_{rel}$ - kaznujmo modele, ki niso homomomorfizmi\n",
    "Naj bo $G=<S|R>$ in $r \\in R$ beseda v generatorjih iz $R$. Na vsak generator lahko gledamo kot na slučajno funkcijo nad $[n]$. Podobno lahko tudi na $r$ gledamo kot na kompozitum svojih generatorjev. Velja še, da je matrika porazdelitve za $r$ enaka $P_r = \\begin{bmatrix} P(r(i) = j) \\end{bmatrix}_{i,j} = \\prod _{s \\in r} P_s$ produkt matrik porazdelitev generatorjev v $r$. \n",
    "\n",
    "Želimo si, da je $r$ kot slučajna funkcija skoraj zagotovo enaka identiteti, torej da je \n",
    "$$\n",
    "P(r = \\text{id}) = 1.\n",
    "$$\n",
    "Ekvivalentno: \n",
    "$$\n",
    "0 = \\log(P(r = \\text{id})) = \\log(\\prod_{i=1}^n r_{i,i}) = \\sum_{i=1}^n \\log (r_{i,j}) = \\text{trace} (\\log(P_r)),\n",
    "$$\n",
    "kjer logaritem matrike računamo po elementih.\n",
    "Za funckijo izgube lahko vzamemo\n",
    "\n",
    "$\\mathcal L_{rel} = \\text{trace} (\\log(P_r)) =\n",
    " \\text{trace} (\\log(\\prod_{s \\in r}  \\text{softmax}(\\phi_s)  ))$.\n",
    "\n",
    "*Na nek način gre za maximal negative likelihood*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df33d7a",
   "metadata": {},
   "source": [
    "### $\\mathcal L_{perm}$ - kaznujmo modele, ki niso permutacije\n",
    "Podobno kot pri upodobitvah, ko definiramo funckijo izgube $\\mathcal L_{unit}$, ki matrike optimizira v unitarne in s tem v obrnljive, lahko tukaj pomagamo $\\mathcal L_{rel}$ s tem, da funckije \"potiskamo\" v bijekcije. \n",
    "\n",
    "Naj bo $s \\in S$ generator. Radi bi, da je $s$ kot preslikava nad $[n]$ skoraj zagotovo bijekcija, torej \n",
    "$$\n",
    "P(s \\in S_n )= 1.\n",
    "$$\n",
    "Velja pa \n",
    "$$\n",
    "P(s \\in S_n )= \\sum_{\\sigma \\in S_n} P(s = \\sigma) = \\sum_{\\sigma \\in S_n}   \\prod_{i=1}^n s_{i, \\sigma(i)}\n",
    "= \\text{Perm} (P_s).\n",
    "$$\n",
    "[Edine stohastične matrike z enotskim permanentom so permutacijske](https://math.stackexchange.com/questions/5063254/if-a-stochastic-matrix-has-unit-permanent-is-it-a-permutation-matrix), torej je \n",
    "$\n",
    "P(s \\in S_n )= 1.\n",
    "$ natantko tedaj, ko je $P_s$ permutacijska. Za stohastične matrike pa je to ekvivalentno temu, da je $P_s$ **unitarna**. \n",
    "\n",
    "Za funkcijo izgube lahko vzamemo kar funckijo izgube za unitarnost, uporabljeno na matrikah $P_s$:\n",
    "$$\n",
    "\\mathcal L_{perm} = \\sum_{s \\in S}||P_sP_s^*  - I||_F^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b79e21",
   "metadata": {},
   "source": [
    "### Avtomorfizmi grafov \n",
    "Naj bo $\\mathcal G = ([n], E)$ graf na $n$ vozljiščih in $M=\\begin{bmatrix}m_{i,j}\\end{bmatrix}$ njegova matrika sosednosti. Naš model $\\rho_\\phi$ lahko spreminjamo v avtomorfizem grafa $\\mathcal G$.\n",
    "\n",
    "Za $(i,j) \\in E$ računamo \n",
    "$$\n",
    "P(i^s \\sim j ^s) = \\sum_{k= 1}^n \\sum_{h = 1}^n s_{i, k} s_{j, h} m_{k,h} = s_j^T M s_i,\n",
    "$$\n",
    "kjer je $P_s = \\begin{bmatrix} s_1^T \\\\ \\vdots \\\\ s_n^T \\end{bmatrix}$.\n",
    "\n",
    "Radi bi, da velja \n",
    "$\n",
    "1 = \\displaystyle\\prod_{(i,j) \\in E} P(i^s \\sim j ^s).\n",
    "$. \n",
    "Za funkcijo izgube lahko vzamemo\n",
    "$$\n",
    "\\mathcal L_{aut} = \\sum_{i=1}^n\\sum_{j=1}^n log(s_j^T M s_i)m_{i,j}= \\text{tr}(\\log(SMS^T) M^T),\n",
    "$$\n",
    "kjer je $\\log(SMS^T)$ izračunan po elementih."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26c7b92",
   "metadata": {},
   "source": [
    "## Strojnoučenjaški pristop\n",
    "Namesto, da optimiziramo model $\\rho \\colon G \\to \\text{fun}([n], [n])$, naredimo model, ki že v začetku slika v $S_n$. (Podobno, kot smo pri upodobitvah naredili model, ki direktno slika v O(2)).\n",
    "\n",
    "Velja $S_n = <a, b | R>$. Vsaka permutacija je torej beseda v $\\{a, b\\}. \n",
    "\n",
    "Besede lahko gradimo rekurzivno. Definiramo model $M : S_n \\to \\text{bernulli}\\{a, b\\}$, ki za vsako permutacijo $\\pi \\in S_n$ vrne vektor verjetnosti  $[P(a | \\pi), P(b | \\pi), P(id | \\pi)]$. S pomočjo tega modela lahko gradimo markovsko verigo:\n",
    "- začneš z identiteto $\\pi_0 = \\text{id}$\n",
    "- vsak korak iz porazdelitve  $[P(a | \\pi_i), P(b | \\pi_i), P(id | \\pi)]$ vzorčiš generator $g \\in \\{a, b\\}$. Če je $g$ identiteta, končaš in vrneš $\\pi _i$, sicer pa nastaviš $\\pi_{i+1} = \\text{perm}(\\pi_i \\circ g)$,\n",
    "kjer je $\\text{perm}$ preslikava, ki besede slika v permutacije, ki jih besede predstavljajo (to je boljše, kot da model za vhod vzame besedo - besede so ppoljubno dolge in redundantne, permutacija pa je  vektor dimenzije $n$).\n",
    "\n",
    "Za poljubno funckijo izgube $\\mathcal L$ nad preslikavami  $G \\mapsto S_n$ lahko minimaliziramo $E[\\mathcal L (\\rho)]$, kjer je $\\rho$ zgoraj opisani slučjani proces.\n",
    "\n",
    "Ta pristop je kul, ker se je z njim (v obliki $S \\to aS  |bs | 1$) vse začelo."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
